
%Works cited (WIP)

@misc{natureAcceleratingMovement,
author = {},
title = {{A}ccelerating eye movement research via accurate and affordable smartphone eye tracking - {N}ature {C}ommunications --- nature.com},
howpublished = {\url{https://www.nature.com/articles/s41467-020-18360-5#:~:text=With%20the%20fixed%20device%20stand,0.03%20cm%20on%20Tobii%20vs}},
year = {},
note = {[Accessed 25-10-2024]},
}
@article{doi:10.4015/S1016237206000476,
author = {SU, MU-CHUN and WANG, KUO-CHUNG and CHEN, GWO-DONG},
title = {AN EYE TRACKING SYSTEM AND ITS APPLICATION IN AIDS FOR PEOPLE WITH SEVERE DISABILITIES},
journal = {Biomedical Engineering: Applications, Basis and Communications},
volume = {18},
number = {06},
pages = {319-327},
year = {2006},
doi = {10.4015/S1016237206000476},

URL = { 
    
        https://doi.org/10.4015/S1016237206000476
    
    

},
eprint = { 
    
        https://doi.org/10.4015/S1016237206000476
    
    

}
,
    abstract = { The object of this paper is to present a set of techniques integrated into a low-lost eye tracking system. Eye tracking systems have many potential applications such as learning emotion monitoring systems, drivers' fatigue detection systems, etc. In this paper, we report how we use an eye tracking system to implement an "eye mouse" to provide computer access for people with severe disabilities. The proposed eye mouse allows people with severe disabilities to use their eye movements to manipulate computers. It requires only one low-cost Web camera and a personal computer. A five-stage algorithm is developed to estimate the directions of eye movements and then use the direction information to manipulate the computer. Several experiments were conducted to test the performance of the eye tracking system. }
}
@ARTICLE{CHIN20081,
author = {Craig {Chin}, Armando {Barreto}, J. Gualberto {Cremades}, Malek {Adjouadi}},
title = {{I}ntegrated electromyogram and eye-gaze tracking cursor control system for computer users with motor disabilities},
howpublished = {\url{https://digitalcommons.fiu.edu/cgi/viewcontent.cgi?article=1026&context=ece_fac}},
year = {2008},
note = {[Accessed 25-10-2024]},
}

@article{KIM201482,
title = {Quadcopter flight control using a low-cost hybrid interface with EEG-based classification and eye tracking},
journal = {Computers in Biology and Medicine},
volume = {51},
pages = {82-92},
year = {2014},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2014.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0010482514001061},
author = {Byung Hyung Kim and Minho Kim and Sungho Jo},
keywords = {Hybrid interface, Brain–computer interface, Mental concentration, Eye tracking, Quadcopter flight control},
abstract = {We propose a wearable hybrid interface where eye movements and mental concentration directly influence the control of a quadcopter in three-dimensional space. This noninvasive and low-cost interface addresses limitations of previous work by supporting users to complete their complicated tasks in a constrained environment in which only visual feedback is provided. The combination of the two inputs augments the number of control commands to enable the flying robot to travel in eight different directions within the physical environment. Five human subjects participated in the experiments to test the feasibility of the hybrid interface. A front view camera on the hull of the quadcopter provided the only visual feedback to each remote subject on a laptop display. Based on the visual feedback, the subjects used the interface to navigate along pre-set target locations in the air. The flight performance was evaluated by comparing with a keyboard-based interface. We demonstrate the applicability of the hybrid interface to explore and interact with a three-dimensional physical space through a flying robot.}
}

@article{DESANTIS20091,
title = {Robust real time eye tracking for computer interface for disabled people},
journal = {Computer Methods and Programs in Biomedicine},
volume = {96},
number = {1},
pages = {1-11},
year = {2009},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2009.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169260709001163},
author = {Alberto {De Santis} and Daniela Iacoviello},
keywords = {Human computer interface, Eye tracking, Image segmentation, Level set, Recursive estimation},
abstract = {Gaze is a natural input for a Human Computer Interface (HCI) for disabled people, who have of course an acute need for a communication system. An efficient eye tracking procedure is presented providing a non-invasive method for real time detection of a subject eyes in a sequence of frames captured by low cost equipment. The procedure can be easily adapted to any subject and is adequately insensitive to changing of the illumination. The eye identification is performed on a piece-wise constant approximation of the frames. It is based on a discrete level set formulation of the variational approach to the optimal segmentation problem. This yields a simplified version of the original data retaining all the information relevant to the application. Tracking is obtained by a fast update of the optimal segmentation between successive frames. No eye movement model is required being the procedure fast enough to obtain the current frame segmentation as one step update from the previous frame segmentation.},
}

@article{doi:10.1177/1545968315575611,
author = {Emanuele Pasqualotto and Tamara Matuz and Stefano Federici and Carolin A. Ruf and Mathias Bartl and Marta Olivetti Belardinelli and Niels Birbaumer and Sebastian Halder},
title ={Usability and Workload of Access Technology for People With Severe Motor Impairment: A Comparison of Brain-Computer Interfacing and Eye Tracking},

journal = {Neurorehabilitation and Neural Repair},
volume = {29},
number = {10},
pages = {950-957},
year = {2015},
doi = {10.1177/1545968315575611},
    note ={PMID: 25753951},

URL = { 
    
        https://doi.org/10.1177/1545968315575611
    
    

},
eprint = { 
    
        https://doi.org/10.1177/1545968315575611
    
    

}
,
    abstract = { Background. Eye trackers are widely used among people with amyotrophic lateral sclerosis, and their benefits to quality of life have been previously shown. On the contrary, Brain-computer interfaces (BCIs) are still quite a novel technology, which also serves as an access technology for people with severe motor impairment. Objective. To compare a visual P300-based BCI and an eye tracker in terms of information transfer rate (ITR), usability, and cognitive workload in users with motor impairments. Methods. Each participant performed 3 spelling tasks, over 4 total sessions, using an Internet browser, which was controlled by a spelling interface that was suitable for use with either the BCI or the eye tracker. At the end of each session, participants evaluated usability and cognitive workload of the system. Results. ITR and System Usability Scale (SUS) score were higher for the eye tracker (Wilcoxon signed-rank test: ITR T = 9, P = .016; SUS T = 12.50, P = .035). Cognitive workload was higher for the BCI (T = 4; P = .003). Conclusions. Although BCIs could be potentially useful for people with severe physical disabilities, we showed that the usability of BCIs based on the visual P300 remains inferior to eye tracking. We suggest that future research on visual BCIs should use eye tracking–based control as a comparison to evaluate performance or focus on nonvisual paradigms for persons who have lost gaze control. }

